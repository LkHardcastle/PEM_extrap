install.packages(c("ggplot2", "cowplot", "tidyr", "dplyr"))
library(ggplot2)
install.packages("KFAS")
library("KFAS")
library("KFAS")
library("discSurv")
setwd("C:\\Users\\hardc\\Documents\\PEM_extrap\\Scripts\\Colon")
library(dplyr)
library(tidyverse)
library(rstan)
library(ggplot2)
#install.packages("survextrap", repos=c('https://chjackson.r-universe.dev',
#                                       'https://cloud.r-project.org'))
library(survextrap)
?fitSSM
colons <- arrange(colons, years)
colons$eventtime <- as.integer(colons$years*12) + 1
ltcolons <- lifeTable(as.data.frame(colons), timeColumn = "eventtime", eventColumn = "status")
ltHaz <- data.frame(hazKM = ltcolons$Output$hazard, Time = (seq(1:length(ltcolons$Output[,1]))-0.5)/12,
AtRisk = ltcolons$Output$atRisk, Events = ltcolons$Output$events)
View(ltHaz)
View(ltcolons)
View(ltcolons$Output)
colons <- arrange(colons, years)
colons$eventtime <- as.integer(colons$years*6) + 1
ltcolons <- lifeTable(as.data.frame(colons), timeColumn = "eventtime", eventColumn = "status")
ltHaz <- data.frame(hazKM = ltcolons$Output$hazard, Time = (seq(1:length(ltcolons$Output[,1]))-0.5)/6,
AtRisk = ltcolons$Output$atRisk, Events = ltcolons$Output$events)
colons <- arrange(colons, years)
colons$eventtime <- as.integer(colons$years*3) + 1
ltcolons <- lifeTable(as.data.frame(colons), timeColumn = "eventtime", eventColumn = "status")
ltHaz <- data.frame(hazKM = ltcolons$Output$hazard, Time = (seq(1:length(ltcolons$Output[,1]))-0.5)/3,
AtRisk = ltcolons$Output$atRisk, Events = ltcolons$Output$events)
ltHaz <- data.frame(hazKM = ltcolons$Output$hazard, Time = (seq(1:length(ltcolons$Output[,1]))-0.5)/12,
AtRisk = ltcolons$Output$atRisk, Events = ltcolons$Output$events)
colons <- arrange(colons, years)
colons$eventtime <- as.integer(colons$years*12) + 1
ltcolons <- lifeTable(as.data.frame(colons), timeColumn = "eventtime", eventColumn = "status")
ltHaz <- data.frame(hazKM = ltcolons$Output$hazard, Time = (seq(1:length(ltcolons$Output[,1]))-0.5)/12,
AtRisk = ltcolons$Output$atRisk, Events = ltcolons$Output$events)
dpois(1,0.005)
dpois(0,0.005)
dpois(5,0.02)
nd_modr1 <- survextrap(Surv(years, status) ~ 1, data=colons, chains=2,
smooth_model = "random_walk",
mspline = list(add_knots=4))
spline_out1 = hazard(nd_modr1, t = seq(0.01, 15, .01))
plot(nd_modr, tmax=5)
# Save
write.csv(spline_out1, "C:\\Users\\hardc\\Documents\\PEM_extrap\\data\\ColonSmps\\spline.csv")
write.csv(spline_out2, "C:\\Users\\hardc\\Documents\\PEM_extrap\\data\\ColonSmps\\spline_ext.csv")
extdat <- data.frame(start = c(10), stop =  c(15),
n = c(20), r = c(6))
nd_modr2 <- survextrap(Surv(years, status) ~ 1, data=colons, chains=2,
smooth_model = "random_walk", external = extdat)
spline_out2 = hazard(nd_modr2, t = seq(0.01, 15, .01))
write.csv(spline_out2, "C:\\Users\\hardc\\Documents\\PEM_extrap\\data\\ColonSmps\\spline_ext.csv")
# Save
write.csv(spline_out1, "C:\\Users\\hardc\\Documents\\PEM_extrap\\data\\ColonSmps\\spline.csv")
library("KFAS")
library("discSurv")
colons <- arrange(colons, years)
colons$eventtime <- as.integer(colons$years*12) + 1
ltcolons <- lifeTable(as.data.frame(colons), timeColumn = "eventtime", eventColumn = "status")
ltHaz <- data.frame(hazKM = ltcolons$Output$hazard, Time = (seq(1:length(ltcolons$Output[,1]))-0.5)/12,
AtRisk = ltcolons$Output$atRisk, Events = ltcolons$Output$events)
# Final month
follow_up <- max(colons$eventtime)
# The above hazard is the product-limit (KM) estimate. Also calculate the life-table (acturial) estimate
ltHaz$hazLT = ltHaz$Events / (ltHaz$AtRisk - ltHaz$Events/2)
# Generate log-time
ltHaz$lnTime <- log(ltHaz$Time)
# For random effects add an ID for each time period
ltHaz$MyId <- 1:dim(ltHaz)[1] # Generate id variable
# For AR(1) model get outcomes lagged by one.
ltHaz$EventsL <- lag(ltHaz$Events)
# Set first lagged value = 0 (usually would discard, but retain so IC are comparable. Can be justified as a prior value)
ltHaz$EventsL[1] <- 0
logTime <- data.frame(matrix(nrow=dim(ltHaz)[1], ncol=4))
colnames(logTime) <- c("Time","Events","AtRisk","Hazard")
# First get equi-spaced in log-time
MyMin <- log(min(ltHaz$Time))
MyMax <- log(max(ltHaz$Time))
MyNum <- follow_up # Number of points we want
logTime$Time <- MyMin+(MyMax-MyMin)/MyNum*(index(logTime)-1)
library(zoo)
library("KFAS")
library("discSurv")
colons <- arrange(colons, years)
colons$eventtime <- as.integer(colons$years*12) + 1
ltcolons <- lifeTable(as.data.frame(colons), timeColumn = "eventtime", eventColumn = "status")
ltHaz <- data.frame(hazKM = ltcolons$Output$hazard, Time = (seq(1:length(ltcolons$Output[,1]))-0.5)/12,
AtRisk = ltcolons$Output$atRisk, Events = ltcolons$Output$events)
# Final month
follow_up <- max(colons$eventtime)
# The above hazard is the product-limit (KM) estimate. Also calculate the life-table (acturial) estimate
ltHaz$hazLT = ltHaz$Events / (ltHaz$AtRisk - ltHaz$Events/2)
# Generate log-time
ltHaz$lnTime <- log(ltHaz$Time)
# For random effects add an ID for each time period
ltHaz$MyId <- 1:dim(ltHaz)[1] # Generate id variable
# For AR(1) model get outcomes lagged by one.
ltHaz$EventsL <- lag(ltHaz$Events)
# Set first lagged value = 0 (usually would discard, but retain so IC are comparable. Can be justified as a prior value)
ltHaz$EventsL[1] <- 0
logTime <- data.frame(matrix(nrow=dim(ltHaz)[1], ncol=4))
colnames(logTime) <- c("Time","Events","AtRisk","Hazard")
# First get equi-spaced in log-time
MyMin <- log(min(ltHaz$Time))
MyMax <- log(max(ltHaz$Time))
MyNum <- follow_up # Number of points we want
logTime$Time <- MyMin+(MyMax-MyMin)/MyNum*(index(logTime)-1)
# Now exponentiate
logTime$Time <- exp(logTime$Time)
logTime$Events <- round(approx(ltHaz$Time,ltHaz$Events, xout=logTime$Time)$y, digits=0)
logTime$AtRisk <- approx(ltHaz$Time,ltHaz$AtRisk, xout=logTime$Time)$y
logTime$Hazard <- (logTime$Events / logTime$AtRisk)
strcLvl <- SSModel(logTime$Events ~ -1 +
SSMtrend(degree = 1, Q = list(matrix(NA))),
distribution="poisson", u=logTime$AtRisk)
strcTrnd <- SSModel(logTime$Events ~ -1 +
SSMtrend(degree = 2, Q = list(matrix(NA), matrix(NA))),
distribution="poisson", u=logTime$AtRisk)
strcDrft <- SSModel(logTime$Events ~ -1 +
SSMtrend(degree = 2, Q = list(matrix(NA), matrix(0))),
distribution="poisson", u=logTime$AtRisk)
# Now fit and get estimates (N.B. for replication using nsim = 0, that is Gaussian approximation only)
modLvl <- fitSSM(strcLvl, c(0.1), method = "BFGS")$model
estLvl <- KFS(modLvl, nsim = 0)
modTrnd <- fitSSM(strcTrnd, c(0.1,0.1), method = "BFGS")$model
estTrnd <- KFS(modTrnd, nsim = 0)
modDrft <- fitSSM(strcDrft, c(0.1,0.1), method = "BFGS")$model
estDrft <- KFS(modDrft, nsim = 0)
predLvl <- data.frame(predict(object=modLvl,
newdata = SSModel(ts(matrix(NA, 50*12-follow_up, 1), start = 1) ~ -1 +
SSMtrend(degree = 1, Q = list(matrix(modLvl$Q[1]))),
distribution="poisson", u=1),
type = "response", interval = "confidence", nsim = 0))
predTrnd <- data.frame(predict(object=modTrnd,
newdata = SSModel(ts(matrix(NA, 50*12-follow_up, 1), start = 1) ~ -1 +
SSMtrend(degree = 2, Q = list(matrix(modTrnd$Q[1]), matrix(modTrnd$Q[4]))),
distribution="poisson", u=1),
type = "response", interval = "confidence", nsim = 0))
predDrft <- data.frame(predict(object=modDrft,
newdata = SSModel(ts(matrix(NA, 50*12-follow_up, 1), start = 1) ~ -1 +
SSMtrend(degree = 2, Q = list(matrix(modDrft$Q[1]), matrix(modDrft$Q[4]))),
distribution="poisson", u=1),
type = "response", interval = "confidence", nsim = 0))
# Generate hazard for observed period - again in log-time
estDLM <- data.frame(Level = estLvl$muhat/logTime$AtRisk, Trend = estTrnd$muhat/logTime$AtRisk,
Drift = estDrft$muhat/logTime$AtRisk, logTime = log(logTime$Time), Time = logTime$Time)
step <- estDLM$logTime[2] - estDLM$logTime[1]
# Collate predictions
pred <- data.frame(Level = predLvl$fit, Trend = predTrnd$fit, Drift = predDrft$fit)
# Include time period & change to usual scale
pred$logTime <- estDLM$logTime[dim(estDLM)[1]] + index(pred)*step
pred$Time <- exp(pred$logTime)
# Keep observations for times < 50
pred2 <- subset(pred, Time < 50)
# Append observed and predicted times
dfDGLM <- rbind(estDLM, pred2)
ltHaz$Level <- ltHaz$AtRisk * approx(x=dfDGLM$Time, y=dfDGLM$Level, xout=ltHaz$Time)$y
ltHaz$Trend <- ltHaz$AtRisk * approx(x=dfDGLM$Time, y=dfDGLM$Trend, xout=ltHaz$Time)$y
ltHaz$Drift <- ltHaz$AtRisk * approx(x=dfDGLM$Time, y=dfDGLM$Drift, xout=ltHaz$Time)$y
dfDGLM = dfDGLM %>% select(-logTime) %>% gather(key = "Model", value = "Haz", -Time)
DSM_out <- dfDGLM
write.csv(DSM_out, "C:\\Users\\hardc\\Documents\\PEM_extrap\\data\\ColonSmps\\DSM.csv")
colons <- arrange(colons, years)
colons$eventtime <- as.integer(colons$years*12) + 1
ltcolons <- lifeTable(as.data.frame(colons), timeColumn = "eventtime", eventColumn = "status")
ltHaz <- data.frame(hazKM = ltcolons$Output$hazard, Time = (seq(1:length(ltcolons$Output[,1]))-0.5)/12,
AtRisk = ltcolons$Output$atRisk, Events = ltcolons$Output$events)
# Look at the data - comes from the flexsurv package (arranged by event time)
bc <- arrange(bc, rectime)
#+1 above as integer rounds down, we want to round up
ltBC <- lifeTable(bc, timeColumn = "rectime2", eventColumn = "censrec")
library("flexsurv")    # RPs (also loads survival) and BC case-study
# Look at the data - comes from the flexsurv package (arranged by event time)
bc <- arrange(bc, rectime)
#-----Monthly life table estimates of hazard-----
bc$rectime2 <- as.integer(bc$rectime/(365.24/12)) + 1
#+1 above as integer rounds down, we want to round up
ltBC <- lifeTable(bc, timeColumn = "rectime2", eventColumn = "censrec")
View(bc)
ltBC$Output
view(ltBC$Output)
bc$rectime2 <- as.integer(bc$rectime/(365.24/12)) + 1
#+1 above as integer rounds down, we want to round up
ltBC <- lifeTable(bc, timeColumn = "rectime2", eventColumn = "censrec")
head(ltBC$Output, 5)
ltHaz <- data.frame(hazKM = ltBC$Output$hazard, Time = (seq(1:length(ltBC$Output[,1]))-0.5)/12,
AtRisk = ltBC$Output$atRisk, Events = ltBC$Output$events)
# The above hazard is the product-limit (KM) estimate. Also calculate the life-table (acturial) estimate
ltHaz$hazLT = ltHaz$Events / (ltHaz$AtRisk - ltHaz$Events/2)
# Generate log-time
ltHaz$lnTime <- log(ltHaz$Time)
# For random effects add an ID for each time period
ltHaz$MyId <- 1:dim(ltHaz)[1] # Generate id variable
# For AR(1) model get outcomes lagged by one.
ltHaz$EventsL <- lag(ltHaz$Events)
# Set first lagged value = 0 (usually would discard, but retain so IC are comparable. Can be justified as a prior value)
ltHaz$EventsL[1] <- 0
logTime <- data.frame(matrix(nrow=dim(ltHaz)[1], ncol=4))
colnames(logTime) <- c("Time","Events","AtRisk","Hazard")
# First get equi-spaced in log-time
MyMin <- log(min(ltHaz$Time))
MyMax <- log(max(ltHaz$Time))
MyNum <- follow_up # Number of points we want
logTime$Time <- MyMin+(MyMax-MyMin)/MyNum*(index(logTime)-1)
# Now exponentiate
logTime$Time <- exp(logTime$Time)
logTime$Events <- round(approx(ltHaz$Time,ltHaz$Events, xout=logTime$Time)$y, digits=0)
logTime$AtRisk <- approx(ltHaz$Time,ltHaz$AtRisk, xout=logTime$Time)$y
logTime$Hazard <- (logTime$Events / logTime$AtRisk)
# Now for models. Considering 3:
# modLvl = local level, no trend.
# modTrnd = local level and local trend
# modDrift = local level and global trend
# Define model structures
strcLvl <- SSModel(logTime$Events ~ -1 +
SSMtrend(degree = 1, Q = list(matrix(NA))),
distribution="poisson", u=logTime$AtRisk)
strcTrnd <- SSModel(logTime$Events ~ -1 +
SSMtrend(degree = 2, Q = list(matrix(NA), matrix(NA))),
distribution="poisson", u=logTime$AtRisk)
strcDrft <- SSModel(logTime$Events ~ -1 +
SSMtrend(degree = 2, Q = list(matrix(NA), matrix(0))),
distribution="poisson", u=logTime$AtRisk)
# Now fit and get estimates (N.B. for replication using nsim = 0, that is Gaussian approximation only)
modLvl <- fitSSM(strcLvl, c(0.01), method = "BFGS")$model
follow_up = 88
# Need observations to be equally-spaced in log-time
logTime <- data.frame(matrix(nrow=dim(ltHaz)[1], ncol=4))
colnames(logTime) <- c("Time","Events","AtRisk","Hazard")
# First get equi-spaced in log-time
MyMin <- log(min(ltHaz$Time))
MyMax <- log(max(ltHaz$Time))
MyNum <- follow_up # Number of points we want
logTime$Time <- MyMin+(MyMax-MyMin)/MyNum*(index(logTime)-1)
# Now exponentiate
logTime$Time <- exp(logTime$Time)
logTime$Events <- round(approx(ltHaz$Time,ltHaz$Events, xout=logTime$Time)$y, digits=0)
logTime$AtRisk <- approx(ltHaz$Time,ltHaz$AtRisk, xout=logTime$Time)$y
logTime$Hazard <- (logTime$Events / logTime$AtRisk)
# Now for models. Considering 3:
# modLvl = local level, no trend.
# modTrnd = local level and local trend
# modDrift = local level and global trend
# Define model structures
strcLvl <- SSModel(logTime$Events ~ -1 +
SSMtrend(degree = 1, Q = list(matrix(NA))),
distribution="poisson", u=logTime$AtRisk)
strcTrnd <- SSModel(logTime$Events ~ -1 +
SSMtrend(degree = 2, Q = list(matrix(NA), matrix(NA))),
distribution="poisson", u=logTime$AtRisk)
strcDrft <- SSModel(logTime$Events ~ -1 +
SSMtrend(degree = 2, Q = list(matrix(NA), matrix(0))),
distribution="poisson", u=logTime$AtRisk)
# Now fit and get estimates (N.B. for replication using nsim = 0, that is Gaussian approximation only)
modLvl <- fitSSM(strcLvl, c(0.01), method = "BFGS")$model
estLvl <- KFS(modLvl, nsim = 0)
modTrnd <- fitSSM(strcTrnd, c(0.01,0.01), method = "BFGS")$model
estTrnd <- KFS(modTrnd, nsim = 0)
modDrft <- fitSSM(strcDrft, c(0.01,0.01), method = "BFGS")$model
estDrft <- KFS(modDrft, nsim = 0)
# Get extrapolations (note will be in equally spaced log-time increments)
predLvl <- data.frame(predict(object=modLvl,
newdata = SSModel(ts(matrix(NA, 50*12-follow_up, 1), start = 1) ~ -1 +
SSMtrend(degree = 1, Q = list(matrix(modLvl$Q[1]))),
distribution="poisson", u=1),
type = "response", interval = "confidence", nsim = 0))
predTrnd <- data.frame(predict(object=modTrnd,
newdata = SSModel(ts(matrix(NA, 50*12-follow_up, 1), start = 1) ~ -1 +
SSMtrend(degree = 2, Q = list(matrix(modTrnd$Q[1]), matrix(modTrnd$Q[4]))),
distribution="poisson", u=1),
type = "response", interval = "confidence", nsim = 0))
predDrft <- data.frame(predict(object=modDrft,
newdata = SSModel(ts(matrix(NA, 50*12-follow_up, 1), start = 1) ~ -1 +
SSMtrend(degree = 2, Q = list(matrix(modDrft$Q[1]), matrix(modDrft$Q[4]))),
distribution="poisson", u=1),
type = "response", interval = "confidence", nsim = 0))
# Generate hazard for observed period - again in log-time
estDLM <- data.frame(Level = estLvl$muhat/logTime$AtRisk, Trend = estTrnd$muhat/logTime$AtRisk,
Drift = estDrft$muhat/logTime$AtRisk, logTime = log(logTime$Time), Time = logTime$Time)
# Find the log-times used for extrapolations
# First get the 'step size' for predictions
step <- estDLM$logTime[2] - estDLM$logTime[1]
# Collate predictions
pred <- data.frame(Level = predLvl$fit, Trend = predTrnd$fit, Drift = predDrft$fit)
# Include time period & change to usual scale
pred$logTime <- estDLM$logTime[dim(estDLM)[1]] + index(pred)*step
pred$Time <- exp(pred$logTime)
# Keep observations for times < 50
pred2 <- subset(pred, Time < 50)
# Append observed and predicted times
dfDGLM <- rbind(estDLM, pred2)
View(dfDGLM)
